{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NormalizingFlow_AdvancedML_Verine2022.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyON35jNMRI1E+jRmP0W74Sd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexVerine/AdvancedML_NF/blob/main/NormalizingFlow_AdvancedML_Verine2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Normalizing Flow \n",
        "By : Alexandre Vérine - 18/02/2022 \n",
        "\n",
        "For the Advanved Machine Learning Lectures- Master IASD 2022"
      ],
      "metadata": {
        "id": "cYHI6R4Ax3Fw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Required modules :"
      ],
      "metadata": {
        "id": "upOFQgn9JwPW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIqLciMKxN0S"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available()  else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Coding different implementations"
      ],
      "metadata": {
        "id": "DdVU3d62Ki2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Linear Flow"
      ],
      "metadata": {
        "id": "WqUHmkXlQzsF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Linear Flow is based on: \n",
        "$f_i(\\boldsymbol{x}) = \\boldsymbol{A_i}\\boldsymbol{x}+\\boldsymbol{b_i}$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pDfOj0qELFca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " <font color='red'>**Code to complete :**</font> ✍\n",
        "\n"
      ],
      "metadata": {
        "id": "bMXTozcZM2lY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearBlock(nn.Module):\n",
        "    def __init__(self, input_dimension):\n",
        "        \"\"\"Linear Invertible Block\n",
        "        Args:\n",
        "            input_dimension (int): Number of dimensions in the input.\n",
        "        \"\"\"\n",
        "        super(LinearBlock, self).__init__()\n",
        "        self.A = nn.Parameter(torch.eye(input_dimension).to(device))\n",
        "        self.b = nn.Parameter(torch.zeros((input_dimension)).to(device))\n",
        "        self.update()\n",
        "\n",
        "    def forward(self, x, ldj, reverse = False):\n",
        "        if reverse:\n",
        "            y = F.linear(x, weight=self.Ainv, bias=self.binv)\n",
        "            ldj -= self.ldj\n",
        "        else:\n",
        "            y = F.linear(x, weight=self.A, bias=self.b)\n",
        "            ldj += self.ldj\n",
        "        return y, ldj\n",
        "\n",
        "    def update(self):\n",
        "        self.Ainv = \"\"\" Your code here \"\"\"\n",
        "        self.binv = \"\"\" Your code here \"\"\"\n",
        "        self.ldj = \"\"\" Your code here \"\"\""
      ],
      "metadata": {
        "id": "0O3V8t63L7A8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this cellule to check if your code works : "
      ],
      "metadata": {
        "id": "aFEJ6PzkOuJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = LinearBlock(15)\n",
        "if torch.cuda.is_available():\n",
        "    f = f.cuda()\n",
        "x = torch.randn((100,15)).to(device)\n",
        "ldj = torch.zeros((x.size(0), 1)).to(device)\n",
        "\n",
        "y, ldjo = f(x, ldj)\n",
        "xi, ldji  = f(y, ldjo, True)\n",
        "\n",
        "torch.testing.assert_allclose(xi, x, atol=0.01, rtol=0.01)\n",
        "torch.testing.assert_allclose(ldji, ldjo, atol=0.01, rtol=0.01)\n",
        "\n",
        "del f, x, xi, y, ldj, ldjo, ldji"
      ],
      "metadata": {
        "id": "T3XLq8Q2L7d2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Coupling Functions\n",
        "\n",
        "First we implement the Neural network $g$ defined in the lecture. It can be used either for the translate and/or the scale factor ($g^1$ or $g^2$)."
      ],
      "metadata": {
        "id": "oDvdh3jXQFF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NN(nn.Module):\n",
        "  \"\"\"Small neural network used to compute scale or translate factors.\n",
        "  Args:\n",
        "      input_dimension (int): Number of dimensions in the input.\n",
        "      hidden_dimension (int): Number of dimensions in the hidden layers.\n",
        "      output_dimensiion (int): Number of dimensions in the output.\n",
        "      activation (bool): Use activation.\n",
        "  \"\"\"\n",
        "  def __init__(self, input_dimension, hidden_dimension,\n",
        "               output_dimension, activation=True):\n",
        "    super(NN, self).__init__()\n",
        "\n",
        "    self.activation = activation\n",
        "    self.in_layer = nn.Linear(input_dimension, hidden_dimension)\n",
        "    self.mid_layer1 = nn.Linear(hidden_dimension, hidden_dimension)\n",
        "    self.mid_layer2 = nn.Linear(hidden_dimension, hidden_dimension)\n",
        "    self.out_layer = nn.Linear(hidden_dimension, output_dimension)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.in_layer(x) \n",
        "    if self.activation:\n",
        "        x = F.relu(x)\n",
        "    x = self.mid_layer1(x)\n",
        "    if self.activation:\n",
        "        x = F.relu(x)\n",
        "    x = self.mid_layer2(x)\n",
        "    if self.activation:\n",
        "        x = F.relu(x)\n",
        "    x = self.out_layer(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "OVrACMakSrpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Additive Coupling\n",
        "\n",
        "The Additive Coupling Flow is based on a split of $\\boldsymbol{x} = (\\boldsymbol{x_A},\\boldsymbol{x_B})$. Then : \n",
        "$f_i(\\boldsymbol{x_A},\\boldsymbol{x_B}) = (\\boldsymbol{x_A} + t(\\boldsymbol{x_B}),\\boldsymbol{x_B})$\n"
      ],
      "metadata": {
        "id": "C4LQouAqQqy7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " <font color='red'>**Code to complete :**</font> ✍"
      ],
      "metadata": {
        "id": "t3SE5Mp2OBYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdditiveCouplingBlock(nn.Module):\n",
        "    \"\"\"Additive Coupling Layer. \n",
        "    Split x into xA, xB and return (xA+t(xB), xB). \n",
        "    Args:\n",
        "      input_dimension (int): Number of dimensions in the input.\n",
        "      hidden_dimension (int): Number of dimensions in the hidden layers.\n",
        "      alternate (bool): reverse xA and xB.\n",
        "      activation (bool): use activation in t.\n",
        "    \"\"\"\n",
        "    def __init__(self,input_dimension,\n",
        "                 hidden_dimension, alternate, activation):\n",
        "        super(AdditiveCouplingBlock, self).__init__()\n",
        "        assert input_dimension%2 == 0\n",
        "        self.translate = NN(input_dimension//2, hidden_dimension,\n",
        "                            input_dimension//2, activation)\n",
        "        self.alternate = alternate\n",
        "        \n",
        "    def forward(self, x, ldj, reverse=False):\n",
        "        if self.alternate:\n",
        "            xB, xA = x.chunk(2, dim=1)\n",
        "        else:\n",
        "            xA, xB = x.chunk(2, dim=1)\n",
        "\n",
        "        t = self.translate(xB)\n",
        "        if reverse:\n",
        "            yA = \"\"\" Your code here \"\"\"\n",
        "        else:\n",
        "            yA = \"\"\" Your code here \"\"\"\n",
        "\n",
        "        if self.alternate:\n",
        "            y = torch.cat((xB, yA), dim=1)\n",
        "        else:\n",
        "            y = torch.cat((yA, xB), dim=1)\n",
        "        \n",
        "        ldj = \"\"\" Your code here \"\"\"\n",
        "        return y, ldj\n",
        "\n",
        "    def update(self):\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "bnE6ofY-SsnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this cellule to check if your code works : "
      ],
      "metadata": {
        "id": "lQsJuRlYSJQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = AdditiveCouplingBlock(20, 30, True, True)\n",
        "if torch.cuda.is_available():\n",
        "    f = f.cuda()\n",
        "x = torch.randn((100,20)).to(device)\n",
        "ldj = torch.zeros((x.size(0), 1)).to(device)\n",
        "\n",
        "y, ldjo = f(x, ldj)\n",
        "xi, ldji  = f(y, ldjo, True)\n",
        "\n",
        "torch.testing.assert_allclose(xi, x, atol=0.01, rtol=0.01)\n",
        "torch.testing.assert_allclose(ldji, ldjo, atol=0.01, rtol=0.01)\n",
        "\n",
        "del f, x, xi, y, ldjo, ldj, ldji"
      ],
      "metadata": {
        "id": "G8Agse-SSs__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Affine Coupling\n",
        "\n",
        "The Additive Coupling Flow is based on a split of $\\boldsymbol{x} = (\\boldsymbol{x_A},\\boldsymbol{x_B})$. Then : \n",
        "$f_i(\\boldsymbol{x_A},\\boldsymbol{x_B}) = (s(\\boldsymbol{x_B})\\boldsymbol{x_A} + t(\\boldsymbol{x_B}),\\boldsymbol{x_B})$\n"
      ],
      "metadata": {
        "id": "Mu_C8K3uRtax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " <font color='red'>**Code to complete :**</font> ✍"
      ],
      "metadata": {
        "id": "KVMOMOFxOCG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AffineCouplingBlock(nn.Module):\n",
        "    \"\"\"Affine Coupling Layer. \n",
        "    Split x into xA, xB and return (s(xB)*xA+t(xB), xB). \n",
        "    Args:\n",
        "      input_dimension (int): Number of dimensions in the input.\n",
        "      hidden_dimension (int): Number of dimensions in the hidden layers.\n",
        "      alternate (bool): reverse xA and xB.\n",
        "      activation (bool): use activation in t.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dimension, \n",
        "                 hidden_dimension, alternate, activation):\n",
        "        super(AffineCouplingBlock, self).__init__()\n",
        "        assert input_dimension%2 == 0\n",
        "        self.st = NN(input_dimension//2, hidden_dimension,\n",
        "                            input_dimension, activation)\n",
        "        self.scale = nn.Parameter(torch.ones(input_dimension//2))\n",
        "        self.alternate = alternate\n",
        "        \n",
        "    def forward(self, x, ldj, reverse=False):\n",
        "        if self.alternate:\n",
        "            xB, xA = x.chunk(2, dim=1)\n",
        "        else:\n",
        "            xA, xB = x.chunk(2, dim=1)\n",
        "\n",
        "        st = self.st(xB)\n",
        "        s, t = st[:, 0::2], st[:, 1::2]\n",
        "        s = self.scale * torch.tanh(s)\n",
        "        if reverse:\n",
        "            ldj = \"\"\" Your code here \"\"\"\n",
        "            s = torch.exp(-s)\n",
        "            yA = \"\"\" Your code here \"\"\"\n",
        "        else:\n",
        "            ldj = \"\"\" Your code here \"\"\"\n",
        "            s = torch.exp(s)\n",
        "            yA = \"\"\" Your code here \"\"\"\n",
        "\n",
        "        if self.alternate:\n",
        "            y = torch.cat((xB, yA), dim=1)\n",
        "        else:\n",
        "            y = torch.cat((yA, xB), dim=1)\n",
        "        \n",
        "        return y, ldj\n",
        "\n",
        "    def update(self):\n",
        "        pass"
      ],
      "metadata": {
        "id": "YcVP5SsuSuPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = AffineCouplingBlock(2, 30, True, True)\n",
        "if torch.cuda.is_available():\n",
        "    f = f.cuda()\n",
        "x = torch.randn((10,2)).to(device)\n",
        "ldj = torch.zeros((x.size(0), 1)).to(device)\n",
        "\n",
        "y, ldjo = f(x, ldj)\n",
        "xi, ldji  = f(y, ldjo, True)\n",
        "\n",
        "torch.testing.assert_allclose(xi, x, atol=0.01, rtol=0.01)\n",
        "torch.testing.assert_allclose(ldji, ldj, atol=0.01, rtol=0.01)\n",
        "\n",
        "del f, x, xi, y, ldjo, ldj, ldji"
      ],
      "metadata": {
        "id": "2ztFHJRwFfCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Residual Flow"
      ],
      "metadata": {
        "id": "M_Ra_QrZTpLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the Residual Flow, the first step is to code a Lipschitz Neural Network:\n",
        "Here the parameters is the maximum  Lipschitz constant of the Linear block."
      ],
      "metadata": {
        "id": "bdDFMnEmTyUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LipschitzLinear(nn.Module):\n",
        "    \"\"\"Lipschitz Linear Function\n",
        "    Args:\n",
        "        in_features (int): Number of dimensions of the input.\n",
        "        out_features (int): Number of dimensions of the output.\n",
        "        coeff (float): Lipstchit constant between 0 and 1 stricly.\n",
        "        activation (bool): Use activation.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, coeff=0.90):\n",
        "        super(LipschitzLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.atol = None\n",
        "        self.rtol = None\n",
        "        self.coeff = coeff\n",
        "        self.n_iterations = 10\n",
        "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "\n",
        "        self.bias = nn.Parameter(torch.Tensor(out_features))\n",
        "        self.reset_parameters()\n",
        "\n",
        "        h, w = self.weight.shape\n",
        "        self.register_buffer('scale', torch.tensor(0.))\n",
        "        self.register_buffer('u', \n",
        "                             F.normalize(self.weight.new_empty(h).normal_(0, 1),\n",
        "                                         dim=0))\n",
        "        self.register_buffer('v', \n",
        "                             F.normalize(self.weight.new_empty(w).normal_(0, 1),\n",
        "                                         dim=0))\n",
        "        self.compute_weight(True, 1000)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        if self.bias is not None:\n",
        "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
        "            bound = 1 / math.sqrt(fan_in)\n",
        "            init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "    def compute_weight(self, update=True, \n",
        "                       n_iterations=None, atol=None, rtol=None):\n",
        "        \n",
        "        n_iterations = (self.n_iterations if n_iterations is None \n",
        "                        else n_iterations)\n",
        "        atol = self.atol if atol is None else atol\n",
        "        rtol = self.rtol if rtol is None else atol\n",
        "\n",
        "        if n_iterations is None and (atol is None or rtol is None):\n",
        "            raise ValueError('Need one of n_iteration or (atol, rtol).')\n",
        "\n",
        "        if n_iterations is None:\n",
        "            n_iterations = 20000\n",
        "\n",
        "        u = self.u\n",
        "        v = self.v\n",
        "        weight = self.weight\n",
        "        if update:\n",
        "            with torch.no_grad():\n",
        "                itrs_used = 0.\n",
        "                for _ in range(n_iterations):\n",
        "                    old_v = v.clone()\n",
        "                    old_u = u.clone()\n",
        "                    v = F.normalize(torch.mv(weight.t(), u), dim=0, out=v)\n",
        "                    u = F.normalize(torch.mv(weight, v), dim=0, out=u)\n",
        "                    itrs_used = itrs_used + 1\n",
        "                    if atol is not None and rtol is not None:\n",
        "                        err_u = torch.norm(u - old_u) / (u.nelement()**0.5)\n",
        "                        err_v = torch.norm(v - old_v) / (v.nelement()**0.5)\n",
        "                        tol_u = atol + rtol * torch.max(u)\n",
        "                        tol_v = atol + rtol * torch.max(v)\n",
        "                        if err_u < tol_u and err_v < tol_v:\n",
        "                            break\n",
        "                if itrs_used > 0:\n",
        "                    u = u.clone()\n",
        "                    v = v.clone()\n",
        "                    self.u = u\n",
        "                    self.v = self.v\n",
        "        sigma = torch.dot(u, torch.mv(weight, v))\n",
        "        with torch.no_grad():\n",
        "            self.scale.copy_(sigma)\n",
        "        # soft normalization: only when sigma larger than coeff\n",
        "        factor = torch.max(torch.ones(1).to(weight.device), sigma / self.coeff)\n",
        "        return weight / factor\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight = self.compute_weight(update=False, n_iterations = 5)\n",
        "        return F.linear(x, weight, self.bias)\n"
      ],
      "metadata": {
        "id": "zDpN6o7JJ6L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we stack the Lipschitz Linear Block to implement the Lipschitz Neural Network. "
      ],
      "metadata": {
        "id": "sP3DOmd9UU-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LipscitzNN(nn.Module):\n",
        "    \"\"\"Small Lipschitz neural network.\n",
        "    Args:\n",
        "        input_dimension (int): Number of dimensions in the input.\n",
        "        hidden_dimension (int): Number of dimensions in the hidden layers.\n",
        "        coeff (float): Lipstchit constant between 0 and 1 stricly.\n",
        "        activation (bool): Use activation.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dimension, hidden_dimension, coeff, activation):\n",
        "        super(LipscitzNN, self).__init__()\n",
        "        self.activation = activation\n",
        "        self.in_layer = LipschitzLinear(input_dimension,\n",
        "                                        hidden_dimension, coeff)\n",
        "        self.mid_layer1 = LipschitzLinear(hidden_dimension,\n",
        "                                          hidden_dimension, coeff)\n",
        "        self.mid_layer2 = LipschitzLinear(hidden_dimension,\n",
        "                                          hidden_dimension, coeff)\n",
        "        self.out_layer = LipschitzLinear(hidden_dimension,\n",
        "                                         input_dimension, coeff)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.in_layer(x) \n",
        "        if self.activation:\n",
        "            x = F.relu(x)\n",
        "        x = self.mid_layer1(x)\n",
        "        if self.activation:\n",
        "            x = F.relu(x)\n",
        "        x = self.mid_layer2(x)\n",
        "        if self.activation:\n",
        "            x = F.relu(x)\n",
        "        x = self.out_layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "A7c-fUZ_J5t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally the complete implementation. The function ``` logdetgrad ```  approximates the log determinant of Jacobian Matrix. The ``` inverse ``` uses the Fixed-Point iteration algorithm to compute the inverse. \n",
        "\n"
      ],
      "metadata": {
        "id": "c6voIygaUb5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='red'>**Code to complete :**</font> ✍"
      ],
      "metadata": {
        "id": "GUqYFAUFOIl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Residual Atomic Block function.\n",
        "    Takes x and ldj as the input and returns x+g(x) and ldj + logdetgrad(x)\n",
        "    Args:\n",
        "        input_dimension (int): Number of dimensions in the input.\n",
        "        hidden_dimension (int): Number of dimensions in the hidden layers.\n",
        "        coeff (float): Lipstchit constant between 0 and 1 stricly.\n",
        "        activation (bool): Use activation.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dimension, hidden_dimension, coeff, activation):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.g = LipscitzNN(input_dimension, \n",
        "                            hidden_dimension, \n",
        "                            coeff, \n",
        "                            activation)\n",
        "        \n",
        "    def forward(self, x, ldj, reverse = False):\n",
        "        if reverse:\n",
        "            y = self.inverse_fixed_point(x)\n",
        "            ldj_out = ldj# - self.logdetgrad(x)\n",
        "        else:\n",
        "            y = x + self.g(x)\n",
        "            ldj_out = ldj + self.logdetgrad(x)\n",
        "        return y, ldj_out\n",
        "\n",
        "    def inverse_fixed_point(self, y, atol=1e-5, rtol=1e-5):\n",
        "        x, x_prev = y - self.g(y), y\n",
        "        i = 0\n",
        "        tol = atol + y.abs() * rtol\n",
        "        while not torch.all(torch.abs(x - x_prev) / tol < 1):\n",
        "            x, x_prev = y - self.g(x), x\n",
        "            i += 1\n",
        "            if i > 5000:\n",
        "                break\n",
        "        return x\n",
        "    \n",
        "    def logdetgrad(self, x):\n",
        "        def poisson_sample(lamb, n_samples):\n",
        "            return np.random.poisson(lamb, n_samples)\n",
        "\n",
        "        def poisson_1mcdf(lamb, k, offset):\n",
        "            if k <= offset:\n",
        "                return 1.\n",
        "            else:\n",
        "                k = k - offset\n",
        "            \"\"\"P(n >= k)\"\"\"\n",
        "            s = 1.\n",
        "            for i in range(1, k):\n",
        "                s += lamb**i / math.factorial(i)\n",
        "            return 1 - np.exp(-lamb) * s\n",
        "\n",
        "        def batch_jacobian(g, x):\n",
        "            jac = []\n",
        "            for d in range(g.shape[1]):\n",
        "                jac.append(torch.autograd.grad(torch.sum(g[:, d]), \n",
        "                        x, \n",
        "                        create_graph=True)[0].view(x.shape[0], 1, x.shape[1]))\n",
        "            return torch.cat(jac, 1)\n",
        "\n",
        "        def batch_trace(M):\n",
        "            return M.view(M.shape[0], -1)[:, ::M.shape[1] + 1].sum(1)\n",
        "\n",
        "        n_samples = poisson_sample(2., 10)\n",
        "        coeff_fn = lambda k: 1 / poisson_1mcdf(2., k, 10) * \\\n",
        "              sum(n_samples >= k - 5) / len(n_samples)\n",
        "        x = x.requires_grad_(True)\n",
        "        g = self.g(x)\n",
        "        jac = batch_jacobian(g, x)\n",
        "        logdetgrad = batch_trace(jac)\n",
        "        jac_k = jac\n",
        "        for k in range(2, 2):\n",
        "            jac_k = torch.bmm(jac, jac_k)\n",
        "            logdetgrad = (logdetgrad \n",
        "                        + (-1)**(k + 1) / k * coeff_fn(k) * batch_trace(jac_k))\n",
        "\n",
        "        return logdetgrad.view(-1, 1)\n",
        "\n",
        "    def update(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, LipschitzLinear):\n",
        "                m.compute_weight(update=True, n_iterations=100)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MsRalsWqJ6s-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this cellule to check if your code works :"
      ],
      "metadata": {
        "id": "1SEddZ-fjtED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = ResidualBlock(1, 30, 0.5, True)\n",
        "if torch.cuda.is_available():\n",
        "    f = f.cuda()\n",
        "f.update()\n",
        "x = torch.randn((10,1)).to(device)\n",
        "ldj = torch.zeros((x.size(0),1)).to(device)\n",
        "\n",
        "y, ldjo = f(x, ldj)\n",
        "xi, ldji  = f(y, ldjo, True)\n",
        "\n",
        "torch.testing.assert_allclose(xi, x, atol=0.01, rtol=0.01)\n",
        "torch.testing.assert_allclose(ldji, ldj, atol=0.01, rtol=0.01)\n",
        "del f, x, xi, y, ldjo, ldj, ldji"
      ],
      "metadata": {
        "id": "rT98tLLxJ8an"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can build a class for all different types of Normalizing Flow : "
      ],
      "metadata": {
        "id": "E9gdIWKiWTjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NormalizingFlow(nn.Module):\n",
        "    \"\"\"Normalizing Flow class.\n",
        "    Takes x as input and return F(x) and sldj. If F(y, reverse=True) is \n",
        "    called then it returns the inverse of F. \n",
        "    Args:\n",
        "        input_dimension (int): Number of dimensions in the input.\n",
        "        hidden_dimension (int): Number of dimensions in the hidden layers.\n",
        "        coeff (float): Lipstchit constant between 0 and 1 stricly.\n",
        "        activation (bool): Use activation.\n",
        "    \"\"\"\n",
        "    def __init__(self, atomic='Linear', dimension=1, hidden_dimension=15, \n",
        "                num_steps=10, activation=True, coeff=0.9):\n",
        "        super(NormalizingFlow, self).__init__()\n",
        "        self.dimension = dimension\n",
        "        if atomic == 'Linear':\n",
        "            self.flows = [LinearBlock(input_dimension=dimension)\n",
        "                            for depth in range(num_steps)]\n",
        "        if atomic == 'AffineCoupling':\n",
        "            self.flows = [AffineCouplingBlock(input_dimension=dimension,\n",
        "                                            hidden_dimension=hidden_dimension,\n",
        "                                            alternate = (depth%2==0),\n",
        "                                            activation=activation)\n",
        "                                            for depth in range(num_steps)]\n",
        "        elif atomic == 'AdditiveCoupling':\n",
        "            self.flows = [AdditiveCouplingBlock(input_dimension=dimension,\n",
        "                                            hidden_dimension=hidden_dimension,\n",
        "                                            alternate = (depth%2==0),\n",
        "                                            activation=activation)\n",
        "                                            for depth in range(num_steps)]\n",
        "        elif atomic == 'Residual':\n",
        "            self.flows = [ResidualBlock(input_dimension=dimension,\n",
        "                                            hidden_dimension=hidden_dimension, \n",
        "                                            coeff=coeff,\n",
        "                                            activation=activation)\n",
        "                                            for depth in range(num_steps)]           \n",
        "        self.flows = nn.ModuleList(self.flows)\n",
        "\n",
        "    def forward(self, x, reverse=False):\n",
        "        sldj = torch.zeros((x.size(0), 1)).to(device)\n",
        "        if reverse:\n",
        "            flows = reversed(self.flows)\n",
        "        else:\n",
        "            flows = self.flows\n",
        "        \n",
        "        for block in flows:\n",
        "            x, sldj = block(x, sldj, reverse)\n",
        "        return x, sldj\n",
        "\n",
        "    def update(self):\n",
        "        for block in self.flows:\n",
        "            block.update()\n",
        "    "
      ],
      "metadata": {
        "id": "4eAu0WZ0xXYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this cellule to check if your code works :"
      ],
      "metadata": {
        "id": "24-rOCxCl2FQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for block_type in ['Linear', 'AffineCoupling', 'AdditiveCoupling',\n",
        "                   'Residual']:\n",
        "    print('Testing: '+block_type+'...')\n",
        "    f = NormalizingFlow(atomic=block_type, dimension=10)\n",
        "    if torch.cuda.is_available():\n",
        "        f = f.cuda()\n",
        "    x = torch.randn((5,10)).to(device)\n",
        "    ldj = torch.zeros((x.size(0), 1)).to(device)\n",
        "\n",
        "    y, ldjo = f(x)\n",
        "    xi, ldji  = f(y, True)\n",
        "\n",
        "    torch.testing.assert_allclose(xi, x, atol=0.01, rtol=0.01)\n",
        "    torch.testing.assert_allclose(ldji+ldjo, ldj, atol=0.1, rtol=0.1)\n",
        "    del f, x, xi, y, ldjo, ldj, ldji\n",
        "    print('Test OK!')"
      ],
      "metadata": {
        "id": "e1WTaPrtZ2Rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Function"
      ],
      "metadata": {
        "id": "eTR1deFgpgJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need a loss function to train the models. "
      ],
      "metadata": {
        "id": "DhaMIO5CrhJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='red'>**Code to complete :**</font> ✍"
      ],
      "metadata": {
        "id": "SzNlZxdVriTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(z, logdetjac):\n",
        "    \"\"\" Loss Function. \n",
        "    Takes z with shape (batch size, dimensions) and \n",
        "    logdetjac with shape (batch size, 1) as inputs. \n",
        "    It returns the negative log likelihood.\n",
        "    \"\"\" \n",
        "    loss = \"\"\" Your code here \"\"\"\n",
        "    \"\"\" Your code here \"\"\"\n",
        "    \"\"\" Your code here \"\"\"\n",
        "    \"\"\" Your code here \"\"\"\n",
        "    return \"\"\" Your code here \"\"\"\n"
      ],
      "metadata": {
        "id": "hyx0_73BrhuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this cellule to check if your code works :"
      ],
      "metadata": {
        "id": "IR5r7U4Quu4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "z = torch.randn((batch_size, 25)).to(device)\n",
        "logdetjac = torch.rand((batch_size, 1)).to(device)\n",
        "loss1 = loss_function(z, logdetjac)\n",
        "loss2 = loss_function(z*2+1, 2*logdetjac)\n",
        "assert loss1 < loss2, \"Loss1 should be lower than Loss2\"\n",
        "torch.testing.assert_allclose(loss_function(torch.zeros((256, 784)), \n",
        "                                            torch.zeros((256,1)))\n",
        "                                ,720.44 , atol=0.1, rtol=0.1)\n",
        "del batch_size, z, logdetjac, loss1, loss2"
      ],
      "metadata": {
        "id": "9clAmKi3rjJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training of NF is sensitive to learning rate so we adapt the training the learning rate"
      ],
      "metadata": {
        "id": "A06IoVSyvBTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_lr(optimizer, lr):\n",
        "    lr = lr/5\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    return optimizer, lr"
      ],
      "metadata": {
        "id": "NCnB5Xfxu_OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Dimensional Dataset"
      ],
      "metadata": {
        "id": "7bQ3zQVqxGc4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We train the 4 different type of NF on a simple 2D dataset composed of 4 joined Gaussians "
      ],
      "metadata": {
        "id": "K1D462nPxORx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Gaussians(batch_size):\n",
        "    scale = 4.\n",
        "    centers = [(-1, 0.5),(-0.35, -0.5), (0.35, -0.5), (1, 0.5)]\n",
        "    centers = [(scale * x, scale * y) for x, y in centers]\n",
        "\n",
        "    dataset = []\n",
        "    for i in range(batch_size):\n",
        "        point = np.random.randn(2)*1.2\n",
        "        idx = np.random.randint(4)\n",
        "        center = centers[idx]\n",
        "        point[0] += center[0]\n",
        "        point[1] += center[1]\n",
        "        dataset.append(point)\n",
        "    dataset = np.array(dataset, dtype=\"float32\")\n",
        "    dataset /= 1.414\n",
        "    return torch.Tensor(dataset)\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "x  = Gaussians(5000).numpy()\n",
        "plt.hist2d(x[:,0], x[:,1],\n",
        "               range=[[-5, 5], [-5, 5]], bins=100, cmap='inferno')\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.xlim(-5, 5)\n",
        "plt.ylim(-5, 5)\n",
        "plt.show()\n",
        "del x"
      ],
      "metadata": {
        "id": "sQy59SHzxNvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a function to plot the dataset $X_r$, the image of the dataset by the Normalizing Flow $F(X_r)$ that should be normally distributed, the target Gaussian $Z_g$ and the inverse image of the Gaussian $F^{-1}(Z_g)$ that should look like the dataset. "
      ],
      "metadata": {
        "id": "gq_3LaVDx-AU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_2D(model, sample,  device, epoch):\n",
        "    plt.clf()\n",
        "    plt.figure(1, figsize=(7,7))\n",
        "    plt.suptitle(\"Epoch \"+str(epoch))\n",
        "    plt.subplot(2,2,2)\n",
        "    plt.title(\"$F^{-1}(Z_g)$\")\n",
        "    z = Variable(torch.randn((1000, 2)).to(device))\n",
        "    x_gen, ldj = model(z,True)\n",
        "    x_gen = x_gen.detach().cpu().numpy()\n",
        "    plt.hist2d(x_gen[:,0], x_gen[:,1],\n",
        "               range=[[-5, 5], [-5, 5]], bins=100, cmap='inferno')\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.xlim(-5, 5)\n",
        "    plt.ylim(-5, 5)\n",
        "    plt.subplot(2,2,1)\n",
        "    plt.title(\"$X_r$\")\n",
        "    x = sample(1000)\n",
        "    plt.hist2d(x[:,0].numpy(), x[:,1].numpy(),\n",
        "               range=[[-5, 5], [-5, 5]], bins=100, cmap='inferno')\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.xlim(-5, 5)\n",
        "    plt.ylim(-5, 5)\n",
        "    plt.subplot(2,2,3)\n",
        "    plt.title('$F(X_r)$')\n",
        "    x = Variable(x.to(device))\n",
        "    z, ldj = model(x)\n",
        "    z = z.detach().cpu().numpy()\n",
        "    plt.hist2d(z[:,0],z[:,1] , \n",
        "               range=[[-5, 5], [-5, 5]], bins=100, cmap='inferno')\n",
        "    plt.xlim(-5, 5)\n",
        "    plt.ylim(-5, 5)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.subplot(2,2,4)\n",
        "    plt.title('$Z_g$')\n",
        "    z = torch.randn((1000, 2))\n",
        "    plt.hist2d(z[:,0].numpy(),z[:,1].numpy(), \n",
        "               range=[[-5, 5], [-5, 5]], bins=100, cmap='inferno')\n",
        "    plt.xlim(-5, 5)\n",
        "    plt.ylim(-5, 5)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n"
      ],
      "metadata": {
        "id": "cm1q7eiUpeTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, x_sample, batch_size):\n",
        "    model = model.cuda()\n",
        "    device = torch.device('cuda:0')\n",
        "    n_epochs = 1000\n",
        "    batch_size = 10000\n",
        "    lr = 0.0001\n",
        "    optimizer = optim.Adam(model.parameters(), lr = lr)\n",
        "    for epoch in range(0, n_epochs+1):\n",
        "        torch.cuda.empty_cache()\n",
        "        x = Variable(x_sample(batch_size)).cuda()\n",
        "        z, logdetjac = model(x)\n",
        "        if epoch %250==0:\n",
        "            plot_2D(model,x_sample, device, epoch)\n",
        "        loss = loss_function(z, logdetjac)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if epoch%5 ==0:\n",
        "            model.update()\n",
        "        if epoch %50 ==0:\n",
        "            print('[%d/%d]: \\tloss: %.3f \\tlr: %.5f' % ((epoch), n_epochs, loss.data.item(), lr))\n",
        "        if epoch in [20, 250, 500,750, 1000]:\n",
        "            optimizer, lr = update_lr(optimizer, lr)\n"
      ],
      "metadata": {
        "id": "es1AmNQ9ydaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NormalizingFlow(\"Linear\", num_steps=4,  dimension = 2)\n",
        "train(model, Gaussians, 1000)"
      ],
      "metadata": {
        "id": "iOUnt6_xyfko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NormalizingFlow(\"AdditiveCoupling\", num_steps=4,  dimension = 2)\n",
        "train(model, Gaussians, 1000)"
      ],
      "metadata": {
        "id": "77QQFgqhyfiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NormalizingFlow(\"AffineCoupling\", num_steps=4,  dimension = 2)\n",
        "train(model, Gaussians, 1000)"
      ],
      "metadata": {
        "id": "6UMI9Uw_yfX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NormalizingFlow(\"Residual\", num_steps=4,  dimension = 2)\n",
        "train(model, Gaussians, 1000)"
      ],
      "metadata": {
        "id": "mXwHjibbyfVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEMEION"
      ],
      "metadata": {
        "id": "WwiwqQ1kyo2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QHCaFUDLqLUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_nD(model, epoch):\n",
        "    z = torch.randn(10, 256).cuda()\n",
        "    x, ldj = model(z, reverse = True)\n",
        "    x = x*0.4697 +0.3286\n",
        "\n",
        "    plt.figure(figsize=(20, 5))\n",
        "    for samples in range(10):\n",
        "        plt.subplot(2, 5, samples+1)\n",
        "        xi = x[samples, :].view(16, 16).detach()\n",
        "        xi = (xi>0.6).float().cpu().numpy()\n",
        "        plt.imshow(xi, cmap='gray')\n",
        "            \n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6pJikjmyqLmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_SEMEION(model, batch_size, lr,  n_epochs):\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "       torchvision.datasets.SEMEION('data/', download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(), \n",
        "                                 torchvision.transforms.Normalize(0.3286,0.4697)\n",
        "                             ])),\n",
        "              batch_size=batch_size, shuffle=True)\n",
        "    model = model.cuda()\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr = lr)\n",
        "    for epoch in range(0, n_epochs+1):\n",
        "        if epoch %20==0:\n",
        "            plot_nD(model, epoch)\n",
        "        for idx, (x,y) in enumerate(train_loader):\n",
        "            torch.cuda.empty_cache()\n",
        "            x = x[y==0, :]\n",
        "            x = Variable(x).cuda()\n",
        "            x = x.view(-1, 256)\n",
        "            z, logdetjac = model(x)\n",
        "            loss = loss_function(z, logdetjac)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if idx%5 ==0:\n",
        "                model.update()\n",
        "        \n",
        "        print('[%d/%d]: loss: %.3f \\tsldj: %.5f' % ((epoch), \n",
        "                                                n_epochs, \n",
        "                                                loss.data.item(),\n",
        "                                                torch.mean(logdetjac).item()))\n",
        "        if epoch in [2, 20, 75, 100, 150]:\n",
        "            optimizer, lr = update_lr(optimizer, lr)\n",
        "    plot_nD(model, epoch)"
      ],
      "metadata": {
        "id": "e2a9s3y42PJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NormalizingFlow(\"Residual\", num_steps=10,  dimension=256, hidden_dimension=512)\n",
        "train_SEMEION(model, 2048, 0.01, 15)"
      ],
      "metadata": {
        "id": "V_WfJn7m2Omc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NormalizingFlow(\"AffineCoupling\", num_steps=10,  dimension=256, hidden_dimension=1024)\n",
        "train_SEMEION(model, 2048, 0.0001, 200)"
      ],
      "metadata": {
        "id": "3tbxfso5-7KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w7rbpVqbPvnw"
      }
    }
  ]
}